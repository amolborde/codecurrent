{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "037950d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "import string\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "from pickle import dump, load\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f59eba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras components for image preprocessing, model creation and layers\n",
    "from keras.applications.xception import Xception, preprocess_input\n",
    "from tensorflow.keras.utils import load_img, img_to_array\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical, get_file\n",
    "from keras.layers import add\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense, LSTM, Embedding, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e2f0f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tqdm for progress bars in loops\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65f5befd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load text file into memory\n",
    "def load_doc(filename):\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aaa99b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map image filenames to their corresponding captions\n",
    "def all_img_captions(filename):\n",
    "    file = load_doc(filename)\n",
    "    captions = file.split('\\n')\n",
    "    descriptions = {}\n",
    "    for caption in captions[:-1]:\n",
    "        img, caption = caption.split('\\t')\n",
    "        if img[:-2] not in descriptions:\n",
    "            descriptions[img[:-2]] = [caption]\n",
    "        else:\n",
    "            descriptions[img[:-2]].append(caption)\n",
    "    return descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2888920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the text captions: lowercase, remove punctuation, numbers etc.\n",
    "def cleaning_text(captions):\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for img, caps in captions.items():\n",
    "        for i, img_caption in enumerate(caps):\n",
    "            img_caption.replace(\"-\", \" \")\n",
    "            desc = img_caption.split()\n",
    "            desc = [word.lower() for word in desc]\n",
    "            desc = [word.translate(table) for word in desc]\n",
    "            desc = [word for word in desc if len(word) > 1]\n",
    "            desc = [word for word in desc if word.isalpha()]\n",
    "            img_caption = ' '.join(desc)\n",
    "            captions[img][i] = img_caption\n",
    "    return captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8be4b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary of all unique words\n",
    "def text_vocabulary(descriptions):\n",
    "    vocab = set()\n",
    "    for key in descriptions.keys():\n",
    "        [vocab.update(d.split()) for d in descriptions[key]]\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "815d33df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save descriptions to file for later use\n",
    "def save_descriptions(descriptions, filename):\n",
    "    lines = list()\n",
    "    for key, desc_list in descriptions.items():\n",
    "        for desc in desc_list:\n",
    "            lines.append(key + '\\t' + desc)\n",
    "    data = \"\\n\".join(lines)\n",
    "    file = open(filename, \"w\")\n",
    "    file.write(data)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "252166a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set dataset paths\n",
    "dataset_text = \"D://ClassAIML//DLWithPython//ClassNB//Projects//ClassProj2//Flickr8k_text\"\n",
    "dataset_images = \"D://ClassAIML//DLWithPython//ClassNB//Projects//ClassProj2//Flicker8k_Dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3c57c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prepare our text data #\n",
    "# filename = dataset_text + \"/\" + \"Flickr8k.token.txt\"\n",
    "# # loading the file that contains all data\n",
    "# # mapping them into descriptions dictionary img to 5 captions\n",
    "# descriptions = all_img_captions(filename)\n",
    "# print(\"Length of descriptions =\" ,len(descriptions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75b4ae75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # cleaning the descriptions\n",
    "# clean_descriptions = cleaning_text(descriptions)\n",
    "\n",
    "# # #building vocabulary \n",
    "# vocabulary = text_vocabulary(clean_descriptions)\n",
    "# print(\"Length of vocabulary = \", len(vocabulary))\n",
    "\n",
    "# # #saving each description to file \n",
    "# save_descriptions(clean_descriptions, \"descriptions.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4bf53e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def download_with_retry(url, filename, max_retries=3):\n",
    "#      for attempt in range(max_retries):\n",
    "#          try:\n",
    "#              return get_file(filename, url)\n",
    "#          except Exception as e:\n",
    "#              if attempt == max_retries - 1:\n",
    "#                  raise e\n",
    "#              print(f\"Download attempt {attempt + 1} failed. Retrying in 5 seconds...\")\n",
    "#              time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d22cd475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Replace the Xception model initialization with:\n",
    "# weights_url = \"https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n",
    "# weights_path = download_with_retry(weights_url, 'xception_weights.h5')\n",
    "# model = Xception(include_top=False, pooling='avg', weights=weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fda06f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_features(directory):\n",
    "#      features = {}\n",
    "#      valid_images = ['.jpg', '.jpeg', '.png']\n",
    "    \n",
    "#      for img in tqdm(os.listdir(directory)):\n",
    "# #        Skip files that don't end with valid image extensions\n",
    "#          ext = os.path.splitext(img)[1].lower()\n",
    "#          if ext not in valid_images:\n",
    "#              continue\n",
    "            \n",
    "#          filename = directory + \"/\" + img\n",
    "#          image = Image.open(filename)\n",
    "#          image = image.resize((299,299))\n",
    "#          image = np.expand_dims(image, axis=0)\n",
    "#          image = image/127.5\n",
    "#          image = image - 1.0\n",
    "\n",
    "#          feature = model.predict(image)\n",
    "#          features[img] = feature\n",
    "#      return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4d0d3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # 2048 feature vector\n",
    "# features = extract_features(dataset_images)\n",
    "# dump(features, open(\"features.p\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b965b76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load features previously saved using Xception model\n",
    "features = load(open(\"features.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a094c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the list of photo filenames\n",
    "def load_photos(filename):\n",
    "    file = load_doc(filename)\n",
    "    photos = file.split(\"\\n\")[:-1]\n",
    "    photos_present = [photo for photo in photos if os.path.exists(os.path.join(dataset_images, photo))]\n",
    "    return photos_present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "adfba6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned captions only for selected photos\n",
    "def load_clean_descriptions(filename, photos): \n",
    "    file = load_doc(filename)\n",
    "    descriptions = {}\n",
    "    for line in file.split(\"\\n\"):\n",
    "        words = line.split()\n",
    "        if len(words) < 1:\n",
    "            continue\n",
    "        image, image_caption = words[0], words[1:]\n",
    "        if image in photos:\n",
    "            if image not in descriptions:\n",
    "                descriptions[image] = []\n",
    "            desc = '<start> ' + \" \".join(image_caption) + ' <end>'\n",
    "            descriptions[image].append(desc)\n",
    "    return descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3be8294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features only for those photos used in training\n",
    "def load_features(photos):\n",
    "    all_features = load(open(\"features.p\", \"rb\"))\n",
    "    features = {k: all_features[k] for k in photos}\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84acc501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training image file names and corresponding descriptions & features\n",
    "filename = dataset_text + \"/\" + \"Flickr_8k.trainImages.txt\"\n",
    "train_imgs = load_photos(filename)\n",
    "train_descriptions = load_clean_descriptions(\"descriptions.txt\", train_imgs)\n",
    "train_features = load_features(train_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "23048942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert descriptions dictionary into a flat list\n",
    "def dict_to_list(descriptions):\n",
    "    all_desc = []\n",
    "    for key in descriptions.keys():\n",
    "        [all_desc.append(d) for d in descriptions[key]]\n",
    "    return all_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "532472fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize all the training captions\n",
    "def create_tokenizer(descriptions):\n",
    "    desc_list = dict_to_list(descriptions)\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(desc_list)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "08f52bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7577\n"
     ]
    }
   ],
   "source": [
    "# Create and save tokenizer\n",
    "tokenizer = create_tokenizer(train_descriptions)\n",
    "dump(tokenizer, open('tokenizer.p', 'wb'))\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9157a3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the maximum length of a caption\n",
    "def max_length(descriptions):\n",
    "    desc_list = dict_to_list(descriptions)\n",
    "    return max(len(d.split()) for d in desc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "812d8691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n"
     ]
    }
   ],
   "source": [
    "max_length = max_length(train_descriptions)\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5d587007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data for training: input features, input sequences, output words\n",
    "def data_generator(descriptions, features, tokenizer, max_length):\n",
    "    def generator():\n",
    "        while True:\n",
    "            for key, description_list in descriptions.items():\n",
    "                feature = features[key][0]\n",
    "                input_image, input_sequence, output_word = create_sequences(tokenizer, max_length, description_list, feature)\n",
    "                for i in range(len(input_image)):\n",
    "                    yield {'input_1': input_image[i], 'input_2': input_sequence[i]}, output_word[i]\n",
    "\n",
    "    output_signature = (\n",
    "        {\n",
    "            'input_1': tf.TensorSpec(shape=(2048,), dtype=tf.float32),\n",
    "            'input_2': tf.TensorSpec(shape=(max_length,), dtype=tf.int32)\n",
    "        },\n",
    "        tf.TensorSpec(shape=(vocab_size,), dtype=tf.float32)\n",
    "    )\n",
    "\n",
    "    dataset = tf.data.Dataset.from_generator(generator, output_signature=output_signature)\n",
    "    return dataset.batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ad000e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each caption to multiple input-output sequence pairs\n",
    "def create_sequences(tokenizer, max_length, desc_list, feature):\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    for desc in desc_list:\n",
    "        seq = tokenizer.texts_to_sequences([desc])[0]\n",
    "        for i in range(1, len(seq)):\n",
    "            in_seq, out_seq = seq[:i], seq[i]\n",
    "            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "            X1.append(feature)\n",
    "            X2.append(in_seq)\n",
    "            y.append(out_seq)\n",
    "    return np.array(X1), np.array(X2), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0eb47cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 2048) (32, 34) (32, 7577)\n"
     ]
    }
   ],
   "source": [
    "# Debug: Print shape of one batch\n",
    "dataset = data_generator(train_descriptions, features, tokenizer, max_length)\n",
    "for (a, b) in dataset.take(1):\n",
    "    print(a['input_1'].shape, a['input_2'].shape, b.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d442f808",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b4b9a713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the image captioning model architecture\n",
    "def define_model(vocab_size, max_length):\n",
    "    # Image feature extractor model input\n",
    "    inputs1 = Input(shape=(2048,), name='input_1')\n",
    "    fe1 = Dropout(0.5)(inputs1)\n",
    "    fe2 = Dense(256, activation='relu')(fe1)\n",
    "\n",
    "    # Sequence model input\n",
    "    inputs2 = Input(shape=(max_length,), name='input_2')\n",
    "    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "    se2 = Dropout(0.5)(se1)\n",
    "    se3 = LSTM(256)(se2)\n",
    "\n",
    "    # Decoder (combining both models)\n",
    "    decoder1 = add([fe2, se3])\n",
    "    decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "\n",
    "    # Define final model\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "    print(model.summary())\n",
    "    plot_model(model, to_file='model.png', show_shapes=True)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8ad0886a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:  6000\n",
      "Descriptions: train= 6000\n",
      "Photos: train= 6000\n",
      "Vocabulary Size: 7577\n",
      "Description Length:  34\n"
     ]
    }
   ],
   "source": [
    "# Print dataset summary\n",
    "print('Dataset: ', len(train_imgs))\n",
    "print('Descriptions: train=', len(train_descriptions))\n",
    "print('Photos: train=', len(train_features))\n",
    "print('Vocabulary Size:', vocab_size)\n",
    "print('Description Length: ', max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d7367d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 34)]         0           []                               \n",
      "                                                                                                  \n",
      " input_1 (InputLayer)           [(None, 2048)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 34, 256)      1939712     ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 2048)         0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 34, 256)      0           ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 256)          524544      ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    (None, 256)          525312      ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 256)          0           ['dense[0][0]',                  \n",
      "                                                                  'lstm[0][0]']                   \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 256)          65792       ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 7577)         1947289     ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5,002,649\n",
      "Trainable params: 5,002,649\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "model = define_model(vocab_size, max_length)\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c0c77e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Estimate number of steps per epoch\n",
    "# def get_steps_per_epoch(train_descriptions):\n",
    "#     total_sequences = 0\n",
    "#     for img_captions in train_descriptions.values():\n",
    "#         for caption in img_captions:\n",
    "#             words = caption.split()\n",
    "#             total_sequences += len(words) - 1\n",
    "#     return max(1, total_sequences // 32)\n",
    "\n",
    "# steps = get_steps_per_epoch(train_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "86b47ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "5/5 [==============================] - 11s 294ms/step - loss: 8.8152\n",
      "Epoch 2/15\n",
      "5/5 [==============================] - 1s 294ms/step - loss: 8.1288\n",
      "Epoch 3/15\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 7.1393\n",
      "Epoch 4/15\n",
      "5/5 [==============================] - 1s 300ms/step - loss: 6.9720\n",
      "Epoch 5/15\n",
      "5/5 [==============================] - 1s 297ms/step - loss: 6.7596\n",
      "Epoch 6/15\n",
      "5/5 [==============================] - 2s 322ms/step - loss: 6.4908\n",
      "Epoch 7/15\n",
      "5/5 [==============================] - 1s 297ms/step - loss: 6.5847\n",
      "Epoch 8/15\n",
      "5/5 [==============================] - 1s 297ms/step - loss: 5.7576\n",
      "Epoch 9/15\n",
      "5/5 [==============================] - 1s 298ms/step - loss: 6.4993\n",
      "Epoch 10/15\n",
      "5/5 [==============================] - 1s 300ms/step - loss: 6.4849\n",
      "Epoch 11/15\n",
      "5/5 [==============================] - 1s 298ms/step - loss: 6.4710\n",
      "Epoch 12/15\n",
      "5/5 [==============================] - 1s 300ms/step - loss: 6.9130\n",
      "Epoch 13/15\n",
      "5/5 [==============================] - 1s 296ms/step - loss: 6.8022\n",
      "Epoch 14/15\n",
      "5/5 [==============================] - 1s 298ms/step - loss: 6.2066\n",
      "Epoch 15/15\n",
      "5/5 [==============================] - 1s 302ms/step - loss: 5.9921\n",
      "Epoch 1/15\n",
      "5/5 [==============================] - 1s 290ms/step - loss: 5.3561\n",
      "Epoch 2/15\n",
      "5/5 [==============================] - 1s 291ms/step - loss: 5.1522\n",
      "Epoch 3/15\n",
      "5/5 [==============================] - 1s 302ms/step - loss: 4.9044\n",
      "Epoch 4/15\n",
      "5/5 [==============================] - 2s 304ms/step - loss: 4.8838\n",
      "Epoch 5/15\n",
      "5/5 [==============================] - 1s 298ms/step - loss: 5.5272\n",
      "Epoch 6/15\n",
      "5/5 [==============================] - 2s 303ms/step - loss: 5.2880\n",
      "Epoch 7/15\n",
      "5/5 [==============================] - 1s 302ms/step - loss: 5.3298\n",
      "Epoch 8/15\n",
      "5/5 [==============================] - 2s 299ms/step - loss: 5.1135\n",
      "Epoch 9/15\n",
      "5/5 [==============================] - 1s 298ms/step - loss: 5.4777\n",
      "Epoch 10/15\n",
      "5/5 [==============================] - 1s 300ms/step - loss: 5.8816\n",
      "Epoch 11/15\n",
      "5/5 [==============================] - 2s 302ms/step - loss: 5.9066\n",
      "Epoch 12/15\n",
      "5/5 [==============================] - 1s 298ms/step - loss: 6.0052\n",
      "Epoch 13/15\n",
      "5/5 [==============================] - 1s 298ms/step - loss: 5.9619\n",
      "Epoch 14/15\n",
      "5/5 [==============================] - 2s 308ms/step - loss: 5.5892\n",
      "Epoch 15/15\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 5.6044\n",
      "Epoch 1/15\n",
      "5/5 [==============================] - 1s 284ms/step - loss: 5.1016\n",
      "Epoch 2/15\n",
      "5/5 [==============================] - 1s 302ms/step - loss: 5.1774\n",
      "Epoch 3/15\n",
      "5/5 [==============================] - 2s 305ms/step - loss: 4.8858\n",
      "Epoch 4/15\n",
      "5/5 [==============================] - 2s 304ms/step - loss: 4.8650\n",
      "Epoch 5/15\n",
      "5/5 [==============================] - 2s 302ms/step - loss: 5.1973\n",
      "Epoch 6/15\n",
      "5/5 [==============================] - 2s 306ms/step - loss: 4.8979\n",
      "Epoch 7/15\n",
      "5/5 [==============================] - 1s 301ms/step - loss: 5.1262\n",
      "Epoch 8/15\n",
      "5/5 [==============================] - 2s 309ms/step - loss: 4.6798\n",
      "Epoch 9/15\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 5.1681\n",
      "Epoch 10/15\n",
      "5/5 [==============================] - 1s 300ms/step - loss: 5.2835\n",
      "Epoch 11/15\n",
      "5/5 [==============================] - 2s 302ms/step - loss: 5.3771\n",
      "Epoch 12/15\n",
      "5/5 [==============================] - 2s 300ms/step - loss: 5.4041\n",
      "Epoch 13/15\n",
      "5/5 [==============================] - 2s 306ms/step - loss: 5.3449\n",
      "Epoch 14/15\n",
      "5/5 [==============================] - 2s 304ms/step - loss: 5.0593\n",
      "Epoch 15/15\n",
      "5/5 [==============================] - 1s 300ms/step - loss: 5.0849\n",
      "Epoch 1/15\n",
      "5/5 [==============================] - 1s 288ms/step - loss: 4.7159\n",
      "Epoch 2/15\n",
      "5/5 [==============================] - 2s 308ms/step - loss: 4.8475\n",
      "Epoch 3/15\n",
      "5/5 [==============================] - 1s 300ms/step - loss: 4.4687\n",
      "Epoch 4/15\n",
      "5/5 [==============================] - 2s 303ms/step - loss: 4.4644\n",
      "Epoch 5/15\n",
      "5/5 [==============================] - 2s 306ms/step - loss: 4.7892\n",
      "Epoch 6/15\n",
      "5/5 [==============================] - 2s 303ms/step - loss: 4.5394\n",
      "Epoch 7/15\n",
      "5/5 [==============================] - 2s 306ms/step - loss: 4.7951\n",
      "Epoch 8/15\n",
      "5/5 [==============================] - 2s 306ms/step - loss: 4.2599\n",
      "Epoch 9/15\n",
      "5/5 [==============================] - 2s 307ms/step - loss: 4.7882\n",
      "Epoch 10/15\n",
      "5/5 [==============================] - 2s 310ms/step - loss: 5.0378\n",
      "Epoch 11/15\n",
      "5/5 [==============================] - 2s 305ms/step - loss: 5.1349\n",
      "Epoch 12/15\n",
      "5/5 [==============================] - 2s 305ms/step - loss: 4.9545\n",
      "Epoch 13/15\n",
      "5/5 [==============================] - 1s 302ms/step - loss: 4.7819\n",
      "Epoch 14/15\n",
      "5/5 [==============================] - 2s 305ms/step - loss: 4.5885\n",
      "Epoch 15/15\n",
      "5/5 [==============================] - 2s 305ms/step - loss: 4.6692\n",
      "Epoch 1/15\n",
      "5/5 [==============================] - 1s 292ms/step - loss: 4.2350\n",
      "Epoch 2/15\n",
      "5/5 [==============================] - 2s 305ms/step - loss: 4.5210\n",
      "Epoch 3/15\n",
      "5/5 [==============================] - 2s 304ms/step - loss: 4.2109\n",
      "Epoch 4/15\n",
      "5/5 [==============================] - 2s 305ms/step - loss: 4.1201\n",
      "Epoch 5/15\n",
      "5/5 [==============================] - 2s 307ms/step - loss: 4.3642\n",
      "Epoch 6/15\n",
      "5/5 [==============================] - 2s 308ms/step - loss: 4.0216\n",
      "Epoch 7/15\n",
      "5/5 [==============================] - 2s 304ms/step - loss: 4.4293\n",
      "Epoch 8/15\n",
      "5/5 [==============================] - 2s 312ms/step - loss: 3.8101\n",
      "Epoch 9/15\n",
      "5/5 [==============================] - 2s 305ms/step - loss: 4.4584\n",
      "Epoch 10/15\n",
      "5/5 [==============================] - 2s 316ms/step - loss: 4.5995\n",
      "Epoch 11/15\n",
      "5/5 [==============================] - 2s 308ms/step - loss: 4.6373\n",
      "Epoch 12/15\n",
      "5/5 [==============================] - 2s 302ms/step - loss: 4.5277\n",
      "Epoch 13/15\n",
      "5/5 [==============================] - 2s 306ms/step - loss: 4.8223\n",
      "Epoch 14/15\n",
      "5/5 [==============================] - 2s 309ms/step - loss: 4.2704\n",
      "Epoch 15/15\n",
      "5/5 [==============================] - 2s 302ms/step - loss: 4.2014\n",
      "Epoch 1/15\n",
      "5/5 [==============================] - 2s 294ms/step - loss: 3.9034\n",
      "Epoch 2/15\n",
      "5/5 [==============================] - 2s 308ms/step - loss: 4.0897\n",
      "Epoch 3/15\n",
      "5/5 [==============================] - 2s 308ms/step - loss: 4.0152\n",
      "Epoch 4/15\n",
      "5/5 [==============================] - 2s 304ms/step - loss: 3.9124\n",
      "Epoch 5/15\n",
      "5/5 [==============================] - 2s 302ms/step - loss: 4.1822\n",
      "Epoch 6/15\n",
      "5/5 [==============================] - 2s 302ms/step - loss: 3.7349\n",
      "Epoch 7/15\n",
      "5/5 [==============================] - 2s 307ms/step - loss: 4.0273\n",
      "Epoch 8/15\n",
      "5/5 [==============================] - 2s 304ms/step - loss: 3.4609\n",
      "Epoch 9/15\n",
      "5/5 [==============================] - 2s 302ms/step - loss: 4.3034\n",
      "Epoch 10/15\n",
      "5/5 [==============================] - 2s 304ms/step - loss: 4.4145\n",
      "Epoch 11/15\n",
      "5/5 [==============================] - 2s 303ms/step - loss: 4.2836\n",
      "Epoch 12/15\n",
      "5/5 [==============================] - 2s 303ms/step - loss: 3.9739\n",
      "Epoch 13/15\n",
      "5/5 [==============================] - 2s 306ms/step - loss: 4.4180\n",
      "Epoch 14/15\n",
      "5/5 [==============================] - 2s 309ms/step - loss: 3.9054\n",
      "Epoch 15/15\n",
      "5/5 [==============================] - 2s 315ms/step - loss: 4.0000\n",
      "Epoch 1/15\n",
      "5/5 [==============================] - 2s 316ms/step - loss: 3.7846\n",
      "Epoch 2/15\n",
      "5/5 [==============================] - 2s 320ms/step - loss: 3.8892\n",
      "Epoch 3/15\n",
      "5/5 [==============================] - 2s 318ms/step - loss: 3.6124\n",
      "Epoch 4/15\n",
      "5/5 [==============================] - 2s 319ms/step - loss: 3.6287\n",
      "Epoch 5/15\n",
      "5/5 [==============================] - 2s 320ms/step - loss: 3.9640\n",
      "Epoch 6/15\n",
      "5/5 [==============================] - 2s 324ms/step - loss: 3.4004\n",
      "Epoch 7/15\n",
      "5/5 [==============================] - 2s 323ms/step - loss: 3.6392\n",
      "Epoch 8/15\n",
      "5/5 [==============================] - 2s 320ms/step - loss: 3.1541\n",
      "Epoch 9/15\n",
      "5/5 [==============================] - 2s 323ms/step - loss: 4.0031\n",
      "Epoch 10/15\n",
      "5/5 [==============================] - 2s 324ms/step - loss: 3.9473\n",
      "Epoch 11/15\n",
      "5/5 [==============================] - 2s 324ms/step - loss: 3.9933\n",
      "Epoch 12/15\n",
      "5/5 [==============================] - 2s 321ms/step - loss: 3.3093\n",
      "Epoch 13/15\n",
      "5/5 [==============================] - 2s 318ms/step - loss: 4.1898\n",
      "Epoch 14/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 2s 322ms/step - loss: 3.6759\n",
      "Epoch 15/15\n",
      "5/5 [==============================] - 2s 320ms/step - loss: 3.5913\n",
      "Epoch 1/15\n",
      "5/5 [==============================] - 2s 321ms/step - loss: 3.2663\n",
      "Epoch 2/15\n",
      "5/5 [==============================] - 2s 321ms/step - loss: 3.4425\n",
      "Epoch 3/15\n",
      "5/5 [==============================] - 2s 312ms/step - loss: 3.2432\n",
      "Epoch 4/15\n",
      "5/5 [==============================] - 2s 325ms/step - loss: 3.2068\n",
      "Epoch 5/15\n",
      "5/5 [==============================] - 2s 318ms/step - loss: 3.5428\n",
      "Epoch 6/15\n",
      "5/5 [==============================] - 2s 320ms/step - loss: 3.1914\n",
      "Epoch 7/15\n",
      "5/5 [==============================] - 2s 320ms/step - loss: 3.2732\n",
      "Epoch 8/15\n",
      "5/5 [==============================] - 2s 323ms/step - loss: 2.9387\n",
      "Epoch 9/15\n",
      "5/5 [==============================] - 2s 318ms/step - loss: 3.7923\n",
      "Epoch 10/15\n",
      "5/5 [==============================] - 2s 320ms/step - loss: 3.8866\n",
      "Epoch 11/15\n",
      "5/5 [==============================] - 2s 323ms/step - loss: 3.9713\n",
      "Epoch 12/15\n",
      "5/5 [==============================] - 2s 320ms/step - loss: 2.7854\n",
      "Epoch 13/15\n",
      "5/5 [==============================] - 2s 312ms/step - loss: 3.7072\n",
      "Epoch 14/15\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 3.2650\n",
      "Epoch 15/15\n",
      "5/5 [==============================] - 2s 327ms/step - loss: 3.7025\n",
      "Epoch 1/15\n",
      "5/5 [==============================] - 2s 315ms/step - loss: 3.1557\n",
      "Epoch 2/15\n",
      "5/5 [==============================] - 2s 320ms/step - loss: 3.0467\n",
      "Epoch 3/15\n",
      "5/5 [==============================] - 2s 320ms/step - loss: 3.0955\n",
      "Epoch 4/15\n",
      "5/5 [==============================] - 2s 315ms/step - loss: 2.9234\n",
      "Epoch 5/15\n",
      "5/5 [==============================] - 2s 321ms/step - loss: 3.2894\n",
      "Epoch 6/15\n",
      "5/5 [==============================] - 2s 318ms/step - loss: 2.9553\n",
      "Epoch 7/15\n",
      "5/5 [==============================] - 2s 309ms/step - loss: 3.0230\n",
      "Epoch 8/15\n",
      "5/5 [==============================] - 2s 309ms/step - loss: 2.6799\n",
      "Epoch 9/15\n",
      "5/5 [==============================] - 2s 311ms/step - loss: 3.4253\n",
      "Epoch 10/15\n",
      "5/5 [==============================] - 2s 317ms/step - loss: 3.2435\n",
      "Epoch 11/15\n",
      "5/5 [==============================] - 2s 312ms/step - loss: 3.7845\n",
      "Epoch 12/15\n",
      "5/5 [==============================] - 2s 312ms/step - loss: 2.4366\n",
      "Epoch 13/15\n",
      "5/5 [==============================] - 2s 312ms/step - loss: 3.6638\n",
      "Epoch 14/15\n",
      "5/5 [==============================] - 2s 308ms/step - loss: 3.1724\n",
      "Epoch 15/15\n",
      "5/5 [==============================] - 2s 311ms/step - loss: 3.5325\n",
      "Epoch 1/15\n",
      "5/5 [==============================] - 2s 308ms/step - loss: 3.0448\n",
      "Epoch 2/15\n",
      "5/5 [==============================] - 2s 308ms/step - loss: 2.8634\n",
      "Epoch 3/15\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 2.8088\n",
      "Epoch 4/15\n",
      "5/5 [==============================] - 2s 316ms/step - loss: 2.6277\n",
      "Epoch 5/15\n",
      "5/5 [==============================] - 2s 305ms/step - loss: 2.9961\n",
      "Epoch 6/15\n",
      "5/5 [==============================] - 2s 314ms/step - loss: 2.6860\n",
      "Epoch 7/15\n",
      "5/5 [==============================] - 2s 310ms/step - loss: 2.7169\n",
      "Epoch 8/15\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 2.4142\n",
      "Epoch 9/15\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 3.2041\n",
      "Epoch 10/15\n",
      "5/5 [==============================] - 2s 312ms/step - loss: 2.9735\n",
      "Epoch 11/15\n",
      "5/5 [==============================] - 2s 303ms/step - loss: 3.3204\n",
      "Epoch 12/15\n",
      "5/5 [==============================] - 2s 309ms/step - loss: 2.3209\n",
      "Epoch 13/15\n",
      "5/5 [==============================] - 2s 310ms/step - loss: 3.1884\n",
      "Epoch 14/15\n",
      "5/5 [==============================] - 2s 311ms/step - loss: 2.8440\n",
      "Epoch 15/15\n",
      "5/5 [==============================] - 2s 308ms/step - loss: 2.9621\n"
     ]
    }
   ],
   "source": [
    "# Train model and save after every epoch\n",
    "os.mkdir(\"avbmodels\")\n",
    "for i in range(epochs):\n",
    "    dataset = data_generator(train_descriptions, train_features, tokenizer, max_length)\n",
    "    model.fit(dataset, epochs=15, steps_per_epoch=5, verbose=1)\n",
    "    model.save(\"avbmodels/model_\" + str(i) + \".h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
